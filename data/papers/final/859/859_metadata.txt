SubmissionNumber#=%=#859
FinalPaperTitle#=%=#Transfer Learning for Low-Resource Neural Machine Translation
ShortPaperTitle#=%=#Transfer Learning for Low-Resource NMT
NumberOfPages#=%=#8
CopyrightSigned#=%=#Jonathan May
JobTitle#==#
Organization#==#USC Information Sciences Institute
4676 Admiralty Way
Suite 1001
Marina Del Rey, CA 90292
Abstract#==#The encoder-decoder framework for neural machine translation (NMT) has been
shown effective in large data scenarios, but is much less effective for
low-resource languages.  We present a transfer learning method that
significantly improves BLEUscores across a range of low-resource languages. 
Our key idea is to first train a high-resource language pair (the parent
model), then transfer some of the learned parameters to the low-resource pair
(the child model) to initialize and constrain training.  Using our transfer
learning method we improve baseline NMT models by an average of 5.6 BLEU on
four low-resource language pairs.  Ensembling and unknown word replacement add
another 2 BLEU which brings the NMT performance on low-resource machine
translation close to a strong syntax based machine translation (SBMT) system,
exceeding its performance on one language pair. Additionally, using the
transfer learning model for re-scoring, we can improve the SBMT system by an
average of 1.3 BLEU, improving the state-of-the-art on low-resource machine
translation.
Author{1}{Firstname}#=%=#Barret
Author{1}{Lastname}#=%=#Zoph
Author{1}{Email}#=%=#barretzoph@gmail.com
Author{1}{Affiliation}#=%=#University of Southern California
Author{2}{Firstname}#=%=#Deniz
Author{2}{Lastname}#=%=#Yuret
Author{2}{Email}#=%=#dyuret@ku.edu.tr
Author{2}{Affiliation}#=%=#Koc University
Author{3}{Firstname}#=%=#Jonathan
Author{3}{Lastname}#=%=#May
Author{3}{Email}#=%=#jonmay@gmail.com
Author{3}{Affiliation}#=%=#USC Information Sciences Institute
Author{4}{Firstname}#=%=#Kevin
Author{4}{Lastname}#=%=#Knight
Author{4}{Email}#=%=#knight@isi.edu
Author{4}{Affiliation}#=%=#USC/ISI

==========