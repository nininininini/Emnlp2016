SubmissionNumber#=%=#26
FinalPaperTitle#=%=#Analyzing the Behavior of Visual Question Answering Models
ShortPaperTitle#=%=#Analyzing the Behavior of VQA Models
NumberOfPages#=%=#6
CopyrightSigned#=%=#Aishwarya Agrawal
JobTitle#==#
Organization#==#Virginia Tech, Blacksburg, VA 24061, United States
Abstract#==#Recently, a number of deep-learning based models have been proposed for the
task of Visual Question Answering (VQA). The performance of most models is
clustered around 60-70%. In this paper we propose systematic methods to analyze
the behavior of these models as a first step towards recognizing their
strengths and weaknesses, and identifying the most fruitful directions for
progress. We analyze two models, one each from two major classes of VQA models
-- with-attention and without-attention and show the similarities and
differences in the behavior of these models. We also analyze the winning entry
of the VQA Challenge 2016.

Our behavior analysis reveals that despite recent progress, today's VQA models
are "myopic" (tend to fail on sufficiently novel instances), often "jump to
conclusions" (converge on a predicted answer after 'listening' to just half the
question), and are "stubborn" (do not change their answers across images).
Author{1}{Firstname}#=%=#Aishwarya
Author{1}{Lastname}#=%=#Agrawal
Author{1}{Email}#=%=#aish@vt.edu
Author{1}{Affiliation}#=%=#Virginia Tech
Author{2}{Firstname}#=%=#Dhruv
Author{2}{Lastname}#=%=#Batra
Author{2}{Email}#=%=#dbatra@vt.edu
Author{2}{Affiliation}#=%=#Virginia Tech
Author{3}{Firstname}#=%=#Devi
Author{3}{Lastname}#=%=#Parikh
Author{3}{Email}#=%=#parikh@vt.edu
Author{3}{Affiliation}#=%=#Georgia Institute of Technology

==========