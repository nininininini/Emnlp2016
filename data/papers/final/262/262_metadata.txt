SubmissionNumber#=%=#262
FinalPaperTitle#=%=#Analyzing Linguistic Knowledge in Sequential Model of Sentence
ShortPaperTitle#=%=#Analyzing Linguistic Knowledge in Sequential Model of Sentence
NumberOfPages#=%=#10
CopyrightSigned#=%=#Peng Qian
JobTitle#==#
Organization#==#
Abstract#==#Sentence modelling is a fundamental topic in computational linguistics.
Recently, deep learning-based sequential models of sentence, such as recurrent
neural network, have proved to be effective in dealing with the non-sequential
properties of human language. However, little is known about how a recurrent
neural network captures linguistic knowledge. Here we propose to correlate the
neuron activation pattern of a LSTM language model with rich language features
at sequential, lexical and compositional level. Qualitative visualization as
well as quantitative analysis under multilingual perspective reveals the
effectiveness of gate neurons and indicates that LSTM learns to allow different
neurons selectively respond to linguistic knowledge at different levels.
Cross-language evidence shows that the model captures different aspects of
linguistic properties for different languages due to the variance of syntactic
complexity. Additionally, we analyze the influence of modelling strategy on
linguistic knowledge encoded implicitly in different sequential models.
Author{1}{Firstname}#=%=#Peng
Author{1}{Lastname}#=%=#Qian
Author{1}{Email}#=%=#pqian11@fudan.edu.cn
Author{1}{Affiliation}#=%=#Fudan University
Author{2}{Firstname}#=%=#Xipeng
Author{2}{Lastname}#=%=#Qiu
Author{2}{Email}#=%=#xpqiu@fudan.edu.cn
Author{2}{Affiliation}#=%=#Fudan University
Author{3}{Firstname}#=%=#Xuanjing
Author{3}{Lastname}#=%=#Huang
Author{3}{Email}#=%=#xjhuang@fudan.edu.cn
Author{3}{Affiliation}#=%=#Fudan University

==========