SubmissionNumber#=%=#140
FinalPaperTitle#=%=#Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention
ShortPaperTitle#=%=#Recognizing Implicit Discourse Relations via Repeated Reading
NumberOfPages#=%=#10
CopyrightSigned#=%=#Yang Liu
JobTitle#==#
Organization#==#Yang Liu, University of Edinburgh, Edinburgh, UK
Abstract#==#Recognizing implicit discourse relations is a challenging but important task in
the field of Natural Language Processing.
For such a complex text processing task, different from previous studies, we
argue that it is necessary to repeatedly read the arguments and dynamically
exploit the efficient features useful for recognizing discourse relations.
To mimic the repeated reading strategy, we propose the neural networks with
multi-level attention (NNMA), combining the attention mechanism and external
memories to gradually fix the attention on some specific words helpful to
judging the discourse relations.
Experiments on the PDTB dataset show that our proposed method achieves the
state-of-art results.
The visualization of the attention weights also illustrates the progress that
our model observes the arguments on each level and progressively locates the
important words.
Author{1}{Firstname}#=%=#Yang
Author{1}{Lastname}#=%=#Liu
Author{1}{Email}#=%=#cs-ly@pku.edu.cn
Author{1}{Affiliation}#=%=#Peking University
Author{2}{Firstname}#=%=#Sujian
Author{2}{Lastname}#=%=#Li
Author{2}{Email}#=%=#lisujian@pku.edu.cn
Author{2}{Affiliation}#=%=#Peking University

==========