SubmissionNumber#=%=#502
FinalPaperTitle#=%=#Language as a Latent Variable: Discrete Generative Models for Sentence Compression
ShortPaperTitle#=%=#Language as a Latent Variable: Discrete Generative Models for Sentence Compression
NumberOfPages#=%=#10
CopyrightSigned#=%=#Yishu Miao
JobTitle#==#
Organization#==#University of Oxford
Abstract#==#In this work we explore deep generative models of text in which the latent
representation of a document is itself drawn from a discrete language model
distribution. We formulate a variational auto-encoder for inference in this
model and apply it to the task of compressing sentences. In this application
the generative model first draws a latent summary sentence from a background
language model, and then subsequently draws the observed sentence conditioned
on this latent summary. In our empirical evaluation we show that generative
formulations of both abstractive and extractive compression yield
state-of-the-art results when trained on a large amount of supervised data.
Further, we explore semi-supervised compression scenarios where we show that it
is possible to achieve performance competitive with previously proposed
supervised models while training on a fraction of the supervised data.
Author{1}{Firstname}#=%=#Yishu
Author{1}{Lastname}#=%=#Miao
Author{1}{Email}#=%=#yishu.miao@cs.ox.ac.uk
Author{1}{Affiliation}#=%=#University of Oxford
Author{2}{Firstname}#=%=#Phil
Author{2}{Lastname}#=%=#Blunsom
Author{2}{Email}#=%=#phil.blunsom@cs.ox.ac.uk
Author{2}{Affiliation}#=%=#University of Oxford

==========