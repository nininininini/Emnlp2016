SubmissionNumber#=%=#889
FinalPaperTitle#=%=#A Decomposable Attention Model for Natural Language Inference
ShortPaperTitle#=%=#A Decomposable Attention Model for Natural Language Inference
NumberOfPages#=%=#7
CopyrightSigned#=%=#Ankur Parikh
JobTitle#==#
Organization#==#Google
Abstract#==#We propose a simple neural architecture for natural language inference. Our
approach uses attention to decompose the problem into subproblems that can be
solved separately, thus making it trivially parallelizable. On the Stanford
Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results
with almost an order of magnitude fewer parameters than previous work and
without relying on any word-order information. Adding intra-sentence attention
that takes a minimum amount of order into account yields further improvements.
Author{1}{Firstname}#=%=#Ankur
Author{1}{Lastname}#=%=#Parikh
Author{1}{Email}#=%=#ankur.p.parikh@gmail.com
Author{1}{Affiliation}#=%=#Google
Author{2}{Firstname}#=%=#Oscar
Author{2}{Lastname}#=%=#Täckström
Author{2}{Email}#=%=#oscar.tackstrom@gmail.com
Author{2}{Affiliation}#=%=#Google
Author{3}{Firstname}#=%=#Dipanjan
Author{3}{Lastname}#=%=#Das
Author{3}{Email}#=%=#dipanjand@gmail.com
Author{3}{Affiliation}#=%=#Google Inc.
Author{4}{Firstname}#=%=#Jakob
Author{4}{Lastname}#=%=#Uszkoreit
Author{4}{Email}#=%=#jakob@uszkoreit.net
Author{4}{Affiliation}#=%=#Google, Inc.

==========