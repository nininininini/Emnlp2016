SubmissionNumber#=%=#207
FinalPaperTitle#=%=#Long Short-Term Memory-Networks for Machine Reading
ShortPaperTitle#=%=#Long Short-Term Memory-Networks for Machine Reading
NumberOfPages#=%=#11
CopyrightSigned#=%=#Jianpeng Cheng
JobTitle#==#
Organization#==#University of Edinburgh
Abstract#==#In this paper we address the question of how to render
          sequence-level networks better at handling structured input.
          We propose a machine reading simulator which processes text
          incrementally from left to right and performs shallow
          reasoning with memory and attention. The reader extends the
          Long Short-Term Memory architecture with a memory network in
          place of a single memory cell. This enables adaptive memory
          usage during recurrence with neural attention, offering a
          way to weakly induce relations among tokens.              The system is
          initially designed to process a single sequence but we also
          demonstrate how to integrate it with an encoder-decoder
          architecture.  Experiments on language modeling, sentiment
          analysis, and natural language inference show that our model
          matches or outperforms the state of the art.
Author{1}{Firstname}#=%=#Jianpeng
Author{1}{Lastname}#=%=#Cheng
Author{1}{Email}#=%=#jianpeng.cheng@ed.ac.uk
Author{1}{Affiliation}#=%=#University of Edinburgh
Author{2}{Firstname}#=%=#Li
Author{2}{Lastname}#=%=#Dong
Author{2}{Email}#=%=#li.dong@ed.ac.uk
Author{2}{Affiliation}#=%=#University of Edinburgh
Author{3}{Firstname}#=%=#Mirella
Author{3}{Lastname}#=%=#Lapata
Author{3}{Email}#=%=#mlap@inf.ed.ac.uk
Author{3}{Affiliation}#=%=#School of Informatics, University of Edinburgh

==========