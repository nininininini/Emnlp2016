SubmissionNumber#=%=#632
FinalPaperTitle#=%=#Deep Reinforcement Learning for Dialogue Generation
ShortPaperTitle#=%=#Deep Reinforcement Learning for Dialogue Generation
NumberOfPages#=%=#11
CopyrightSigned#=%=#jiwei li
JobTitle#==#
Organization#==#Stanford University
Abstract#==#Recent neural models of dialogue generation offer great promise for
generating responses for conversational agents, but tend to be
shortsighted, predicting utterances one at a time while ignoring their
influence on future outcomes.
Modeling the future direction of a dialogue is
crucial to generating coherent, interesting dialogues, a need which led
traditional NLP models of dialogue to draw on reinforcement learning.
In this paper, we show how to integrate these goals,
applying deep reinforcement learning to model future reward in chatbot
dialogue.
The model simulates dialogues between two virtual agents, using policy gradient
methods to reward sequences that display
three useful conversational properties: informativity, coherence, and ease of
answering (related to forward-looking function).
We evaluate our model on diversity, length
as well as with human judges,  showing that the proposed algorithm generates
more interactive responses and manages to foster a more sustained conversation
in dialogue simulation. 
This work marks a first step towards learning a neural conversational model
based on the long-term success of dialogues.
Author{1}{Firstname}#=%=#Jiwei
Author{1}{Lastname}#=%=#Li
Author{1}{Email}#=%=#jiweil@stanford.edu
Author{1}{Affiliation}#=%=#Stanford University
Author{2}{Firstname}#=%=#Will
Author{2}{Lastname}#=%=#Monroe
Author{2}{Email}#=%=#wmonroe4@cs.stanford.edu
Author{2}{Affiliation}#=%=#Stanford University
Author{3}{Firstname}#=%=#Alan
Author{3}{Lastname}#=%=#Ritter
Author{3}{Email}#=%=#ritter.1492@osu.edu
Author{3}{Affiliation}#=%=#The Ohio State University
Author{4}{Firstname}#=%=#Dan
Author{4}{Lastname}#=%=#Jurafsky
Author{4}{Email}#=%=#jurafsky@stanford.edu
Author{4}{Affiliation}#=%=#Stanford University
Author{5}{Firstname}#=%=#Michel
Author{5}{Lastname}#=%=#Galley
Author{5}{Email}#=%=#mgalley@microsoft.com
Author{5}{Affiliation}#=%=#Microsoft Research
Author{6}{Firstname}#=%=#Jianfeng
Author{6}{Lastname}#=%=#Gao
Author{6}{Email}#=%=#jfgao@microsoft.com
Author{6}{Affiliation}#=%=#Microsoft

==========