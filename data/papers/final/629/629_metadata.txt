SubmissionNumber#=%=#629
FinalPaperTitle#=%=#Human Attention in Visual Question Answering: Do Humans and Deep Networks look at the same regions?
ShortPaperTitle#=%=#Human Attention in Visual Question Answering
NumberOfPages#=%=#6
CopyrightSigned#=%=#Abhishek Das
JobTitle#==#
Organization#==#
Abstract#==#We conduct large-scale studies on `human attention' in Visual Question
Answering (VQA) to understand where humans choose to look to answer questions
about images.
We design and test multiple game-inspired novel attention-annotation interfaces
that require the subject to sharpen regions of a blurred image to answer a
question.
Thus, we introduce the VQA-HAT (Human ATtention) dataset.
We evaluate attention maps generated by state-of-the-art VQA models against
human attention both qualitatively (via visualizations) and quantitatively (via
rank-order correlation).
Overall, our experiments show that current VQA attention models do not seem to
be looking at the same regions as humans.
Author{1}{Firstname}#=%=#Abhishek
Author{1}{Lastname}#=%=#Das
Author{1}{Email}#=%=#abhshkdz@vt.edu
Author{1}{Affiliation}#=%=#Virginia Tech
Author{2}{Firstname}#=%=#Harsh
Author{2}{Lastname}#=%=#Agrawal
Author{2}{Email}#=%=#harsh92@vt.edu
Author{2}{Affiliation}#=%=#Virginia Tech
Author{3}{Firstname}#=%=#Larry
Author{3}{Lastname}#=%=#Zitnick
Author{3}{Email}#=%=#zitnick@fb.com
Author{3}{Affiliation}#=%=#Facebook
Author{4}{Firstname}#=%=#Devi
Author{4}{Lastname}#=%=#Parikh
Author{4}{Email}#=%=#parikh@vt.edu
Author{4}{Affiliation}#=%=#Georgia Institute of Technology
Author{5}{Firstname}#=%=#Dhruv
Author{5}{Lastname}#=%=#Batra
Author{5}{Email}#=%=#dbatra@vt.edu
Author{5}{Affiliation}#=%=#Virginia Tech

==========