SubmissionNumber#=%=#352
FinalPaperTitle#=%=#Training with Exploration Improves a Greedy Stack LSTM Parser
ShortPaperTitle#=%=#Training with Exploration Improves a Greedy Stack LSTM Parser
NumberOfPages#=%=#6
CopyrightSigned#=%=#Miguel Ballesteros
JobTitle#==#
Organization#==#UPF
Abstract#==#We adapt the greedy stack LSTM dependency parser of Dyer et al. (2015) to
support a training-with-exploration procedure using dynamic oracles (Goldberg
and Nivre, 2013) instead of assuming an error-free action history. This form of
training, which accounts for
model predictions at training time, improves parsing accuracies. We discuss
some modifications needed in order to get training with exploration to work
well for a probabilistic neural-network dependency parser.
Author{1}{Firstname}#=%=#Miguel
Author{1}{Lastname}#=%=#Ballesteros
Author{1}{Email}#=%=#miguel.ballesteros@upf.edu
Author{1}{Affiliation}#=%=#Pompeu Fabra University
Author{2}{Firstname}#=%=#Yoav
Author{2}{Lastname}#=%=#Goldberg
Author{2}{Email}#=%=#yoav.goldberg@gmail.com
Author{2}{Affiliation}#=%=#Bar Ilan University
Author{3}{Firstname}#=%=#Chris
Author{3}{Lastname}#=%=#Dyer
Author{3}{Email}#=%=#cdyer@google.com
Author{3}{Affiliation}#=%=#Google DeepMind
Author{4}{Firstname}#=%=#Noah A.
Author{4}{Lastname}#=%=#Smith
Author{4}{Email}#=%=#nasmith@cs.washington.edu
Author{4}{Affiliation}#=%=#University of Washington

==========