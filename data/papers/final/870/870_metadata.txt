SubmissionNumber#=%=#870
FinalPaperTitle#=%=#Deep Multi-Task Learning with Shared Memory for Text Classification
ShortPaperTitle#=%=#Deep Multi-Task Learning with Shared Memory for Text Classification
NumberOfPages#=%=#10
CopyrightSigned#=%=#Pengfei Liu
JobTitle#==#
Organization#==#
Abstract#==#Neural network based models have achieved impressive results on various of
natural language processing (NLP) tasks. However, in previous works, most
models are learned separately based on single-task supervised objectives, which
often suffer from insufficient training data. In this paper, we propose two
deep architectures which can be trained jointly on multiple related tasks. More
specifically, we augment neural model with an external memory, which is shared
by several tasks.
Experiments on two groups of text classification tasks show that our proposed
architectures can improve the performance of a task with the help of other
related tasks.
Author{1}{Firstname}#=%=#Pengfei
Author{1}{Lastname}#=%=#Liu
Author{1}{Email}#=%=#pfliu14@fudan.edu.cn
Author{1}{Affiliation}#=%=#
Author{2}{Firstname}#=%=#Xipeng
Author{2}{Lastname}#=%=#Qiu
Author{2}{Email}#=%=#xpqiu@fudan.edu.cn
Author{2}{Affiliation}#=%=#Fudan University
Author{3}{Firstname}#=%=#Xuanjing
Author{3}{Lastname}#=%=#Huang
Author{3}{Email}#=%=#xjhuang@fudan.edu.cn
Author{3}{Affiliation}#=%=#Fudan University

==========