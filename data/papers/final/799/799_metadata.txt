SubmissionNumber#=%=#799
FinalPaperTitle#=%=#Incorporating Discrete Translation Lexicons into Neural Machine Translation
ShortPaperTitle#=%=#Incorporating Discrete Translation Lexicons into Neural Machine Translation
NumberOfPages#=%=#11
CopyrightSigned#=%=#Philip Arthur
JobTitle#==#
Organization#==#
Abstract#==#Neural machine translation (NMT) often makes mistakes in translating
low-frequency content words that are essential to understandng the meaning of
the sentence. We propose a method to alleviate this problem by augmenting NMT
systems with discrete translation lexicons that efficiently encode translations
of these low-frequency words. We describe a method to calculate the lexicon
probability of the next word in the translation candidate by using the
attention vector of the NMT model to select which source word lexical
probabilities the model should focus on. We test two methods to combine this
probability with the standard NMT probability: (1) using it as a bias, and (2)
linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3
BLEU and 0.13-0.44 NIST score, and faster convergence time.
Author{1}{Firstname}#=%=#Philip
Author{1}{Lastname}#=%=#Arthur
Author{1}{Email}#=%=#philip.arthur.om0@is.naist.jp
Author{1}{Affiliation}#=%=#Nara Institute of Science and Technology
Author{2}{Firstname}#=%=#Graham
Author{2}{Lastname}#=%=#Neubig
Author{2}{Email}#=%=#gneubig@cs.cmu.edu
Author{2}{Affiliation}#=%=#Carnegie Mellon University
Author{3}{Firstname}#=%=#Satoshi
Author{3}{Lastname}#=%=#Nakamura
Author{3}{Email}#=%=#s-nakamura@is.naist.jp
Author{3}{Affiliation}#=%=#Nara Institute of Science and Technology

==========