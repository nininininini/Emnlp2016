SubmissionNumber#=%=#527
FinalPaperTitle#=%=#Attention-based LSTM Network for Cross-Lingual Sentiment Classification
ShortPaperTitle#=%=#Attention-based LSTM Network for Cross-Lingual Sentiment Classification
NumberOfPages#=%=#10
CopyrightSigned#=%=#Xinjie Zhou
JobTitle#==#
Organization#==#Institute of Computer Science and Technology, Peking University

128 Zhongguancun North Street, Beijing, China 100080
Abstract#==#Most of the state-of-the-art sentiment classification methods are based on
supervised learning algorithms which require large amounts of manually labeled
data. However, the labeled resources are usually imbalanced in different
languages. Cross-lingual sentiment classification tackles the problem by
adapting the sentiment resources in a resource-rich language to resource-poor
languages. In this study, we propose an attention-based bilingual
representation learning model which learns the distributed semantics of the
documents in both the source and the target languages. In each language, we use
Long Short Term Memory (LSTM) network to model the documents, which has been
proved to be very effective for word sequences. Meanwhile, we propose a
hierarchical attention mechanism for the bilingual LSTM network. The
sentence-level attention model learns which sentences of a document are more
important for determining the overall sentiment while the word-level attention
model learns which words in each sentence are decisive. The proposed model
achieves good results on a benchmark dataset using English as the source
language and Chinese as the target language.
Author{1}{Firstname}#=%=#Xinjie
Author{1}{Lastname}#=%=#Zhou
Author{1}{Email}#=%=#zhouxinjie@pku.edu.cn
Author{1}{Affiliation}#=%=#Peking University
Author{2}{Firstname}#=%=#Xiaojun
Author{2}{Lastname}#=%=#Wan
Author{2}{Email}#=%=#wanxiaojun@pku.edu.cn
Author{2}{Affiliation}#=%=#Peking University
Author{3}{Firstname}#=%=#Jianguo
Author{3}{Lastname}#=%=#Xiao
Author{3}{Email}#=%=#xiaojianguo@pku.edu.cn
Author{3}{Affiliation}#=%=#Peking University

==========