SubmissionNumber#=%=#901
FinalPaperTitle#=%=#Natural Language Model Re-usability for Scaling to Different Domains
ShortPaperTitle#=%=#Natural Language Model Re-usability for Scaling to Different Domains
NumberOfPages#=%=#6
CopyrightSigned#=%=#Young-Bum Kim
JobTitle#==#Senior Research Scientist
Organization#==#Microsoft and Redmond, WA, USA
Abstract#==#Natural language understanding is the core of the human computer interactions.
However, building new domains and tasks that need a separate set of models is a
bottleneck for scaling to a large number of domains and experiences. In this
paper, we propose a practical technique that addresses this issue in a
web-scale language understanding system: Microsoft's personal digital assistant
Cortana. The proposed technique uses a constrained decoding method with a
universal slot tagging model sharing the same schema as the collection of slot
taggers built for each domain. The proposed approach allows reusing of slots
across different domains and tasks while achieving virtually the same
performance as those slot taggers trained per domain fashion.
Author{1}{Firstname}#=%=#Young-Bum
Author{1}{Lastname}#=%=#Kim
Author{1}{Email}#=%=#ybkim@microsoft.com
Author{1}{Affiliation}#=%=#Microsoft
Author{2}{Firstname}#=%=#Alexandre
Author{2}{Lastname}#=%=#Rochette
Author{2}{Email}#=%=#alrochet@microsoft.com
Author{2}{Affiliation}#=%=#Microsoft
Author{3}{Firstname}#=%=#Ruhi
Author{3}{Lastname}#=%=#Sarikaya
Author{3}{Email}#=%=#ruhi_sarikaya@yahoo.com
Author{3}{Affiliation}#=%=#Microsoft

==========