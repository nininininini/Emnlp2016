SubmissionNumber#=%=#3
FinalPaperTitle#=%=#Sparse Non-negative Matrix Language Modeling
ShortPaperTitle#=%=#Sparse Non-negative Matrix Language Modeling
NumberOfPages#=%=#14
CopyrightSigned#=%=#NA
JobTitle#==#
Organization#==#
Abstract#==#We present Sparse Non-negative Matrix (SNM) estimation, a novel probability
estimation technique for language modeling that can efficiently incorporate
arbitrary features. We evaluate SNM language models on two corpora: the One
Billion Word Benchmark and a subset of the LDC English Gigaword corpus. Results
show that SNM language models trained with n-gram features are a close match
for the well-established Kneser-Ney models. The addition of skip-gram features
yields a model that is in the same league as the state-of-the-art recurrent
neural network language models, as well as complementary: combining the two
modeling techniques yields the best known result on the One Billion Word
Benchmark. On the Gigaword corpus further improvements are observed using
features that cross sentence boundaries. The computational advantages of SNM
estimation over both maximum entropy and neural network estimation are probably
its main strength, promising an approach that has large flexibility in
combining arbitrary features and yet scales gracefully to large amounts of
data.
Author{1}{Firstname}#=%=#Joris
Author{1}{Lastname}#=%=#Pelemans
Author{1}{Email}#=%=#joris.pelemans@esat.kuleuven.be
Author{1}{Affiliation}#=%=#KU Leuven
Author{2}{Firstname}#=%=#Noam
Author{2}{Lastname}#=%=#Shazeer
Author{2}{Email}#=%=#noam@google.com
Author{2}{Affiliation}#=%=#Google
Author{3}{Firstname}#=%=#Ciprian
Author{3}{Lastname}#=%=#Chelba
Author{3}{Email}#=%=#ciprianchelba@google.com
Author{3}{Affiliation}#=%=#Google

==========