We introduce a novel approach for detecting and resolving coreference for abstract words. Our approach is completely unsupervised and we also present a new dataset for Biomedical Language Processing which has about {\textasciitilde}25\% of the original corpus vocabulary but still manages to capture the distributional semantic nature of the corpus. Our experiments show that Neural Network models perform much better ({\textasciitilde}20\% more accurate) than the traditional feature-rich baseline models for finding abstract coreferences in biomedical text.
